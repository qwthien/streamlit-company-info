import streamlit as st
import requests
from bs4 import BeautifulSoup
import time
import random
import pandas as pd
import io

# API Key c·ªßa SerpAPI (Thay b·∫±ng API c·ªßa b·∫°n n·∫øu c·∫ßn)
API_KEY = "b8963334c880b8ed0bf2b4219492435b4da63d82c4f13d68fbd7bae4142d4b33"

# Headers gi·∫£ l·∫≠p tr√¨nh duy·ªát
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Referer": "https://www.google.com/",
    "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
}

def get_masothue_link(company_name):
    """T√¨m ki·∫øm c√¥ng ty tr√™n Google ƒë·ªÉ l·∫•y link chu·∫©n t·ª´ masothue.com"""
    search_url = f"https://serpapi.com/search?engine=google&q={company_name}+site:masothue.com&api_key={API_KEY}"
    response = requests.get(search_url)
    results = response.json()

    for result in results.get("organic_results", []):
        if "masothue.com" in result["link"]:
            return result["link"]
    
    return None

def scrape_company_info(company_name, url):
    """Tr√≠ch xu·∫•t th√¥ng tin c√¥ng ty t·ª´ masothue.com"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        st.error(f"‚ö† L·ªói khi truy c·∫≠p {url}: {e}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    company_data = {
        "T√™n c√¥ng ty": company_name,
        "T√™n qu·ªëc t·∫ø": "N/A",
        "T√™n vi·∫øt t·∫Øt": "N/A",
        "M√£ s·ªë thu·∫ø": "N/A",
        "ƒê·ªãa ch·ªâ": "N/A",
        "Ng∆∞·ªùi ƒë·∫°i di·ªán": "N/A",
        "ƒêi·ªán tho·∫°i": "N/A",
        "Ng√†y ho·∫°t ƒë·ªông": "N/A",
        "Qu·∫£n l√Ω b·ªüi": "N/A",
        "Lo·∫°i h√¨nh DN": "N/A",
        "T√¨nh tr·∫°ng": "N/A",
        "Ng√†nh ngh·ªÅ ch√≠nh": "N/A",
    }

    table = soup.find("table", class_="table-taxinfo")
    if not table:
        st.warning(f"‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu tr√™n trang {url}")
        return None

    rows = table.find_all("tr")

    for row in rows:
        columns = row.find_all("td")
        if len(columns) < 2:
            continue  # N·∫øu kh√¥ng c√≥ ƒë·ªß c·ªôt d·ªØ li·ªáu th√¨ b·ªè qua

        label = columns[0].text.strip()
        value = columns[1].text.strip()

        if not label or not value:
            continue

        if "T√™n qu·ªëc t·∫ø" in label:
            company_data["T√™n qu·ªëc t·∫ø"] = value
        elif "T√™n vi·∫øt t·∫Øt" in label:
            company_data["T√™n vi·∫øt t·∫Øt"] = value
        elif "M√£ s·ªë thu·∫ø" in label:
            company_data["M√£ s·ªë thu·∫ø"] = value
        elif "ƒê·ªãa ch·ªâ" in label:
            company_data["ƒê·ªãa ch·ªâ"] = value
        elif "Ng∆∞·ªùi ƒë·∫°i di·ªán" in label:
            company_data["Ng∆∞·ªùi ƒë·∫°i di·ªán"] = value
        elif "ƒêi·ªán tho·∫°i" in label:
            company_data["ƒêi·ªán tho·∫°i"] = value.replace(" ·∫®n th√¥ng tin", "")
        elif "Ng√†y ho·∫°t ƒë·ªông" in label:
            company_data["Ng√†y ho·∫°t ƒë·ªông"] = value
        elif "Qu·∫£n l√Ω b·ªüi" in label:
            company_data["Qu·∫£n l√Ω b·ªüi"] = value
        elif "Lo·∫°i h√¨nh DN" in label:
            company_data["Lo·∫°i h√¨nh DN"] = value
        elif "T√¨nh tr·∫°ng" in label:
            company_data["T√¨nh tr·∫°ng"] = value
        elif "Ng√†nh ngh·ªÅ ch√≠nh" in label:
            company_data["Ng√†nh ngh·ªÅ ch√≠nh"] = value

    return company_data

# Giao di·ªán Streamlit
st.title("üîç Tra c·ª©u th√¥ng tin c√¥ng ty")
st.write("Nh·∫≠p danh s√°ch t√™n c√¥ng ty v√†o b√™n d∆∞·ªõi:")

company_input = st.text_area("Nh·∫≠p danh s√°ch c√¥ng ty (m·ªói d√≤ng m·ªôt c√¥ng ty):")
company_names = [line.strip() for line in company_input.split("\n") if line.strip()]

if st.button("Tra c·ª©u"):
    if not company_names:
        st.warning("Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt t√™n c√¥ng ty!")
    else:
        results = []
        
        for idx, company in enumerate(company_names, start=1):
            st.write(f"üîç ƒêang t√¨m ki·∫øm: **{company}**...")
            url = get_masothue_link(company)

            if url:
                st.success(f"‚úÖ T√¨m th·∫•y: {url}")
                data = scrape_company_info(company, url)
            else:
                st.warning(f"‚ùå Kh√¥ng t√¨m th·∫•y URL cho {company}")
                data = {
                    "T√™n c√¥ng ty": company,
                    "T√™n qu·ªëc t·∫ø": "N/A",
                    "T√™n vi·∫øt t·∫Øt": "N/A",
                    "M√£ s·ªë thu·∫ø": "N/A",
                    "ƒê·ªãa ch·ªâ": "N/A",
                    "Ng∆∞·ªùi ƒë·∫°i di·ªán": "N/A",
                    "ƒêi·ªán tho·∫°i": "N/A",
                    "Ng√†y ho·∫°t ƒë·ªông": "N/A",
                    "Qu·∫£n l√Ω b·ªüi": "N/A",
                    "Lo·∫°i h√¨nh DN": "N/A",
                    "T√¨nh tr·∫°ng": "N/A",
                    "Ng√†nh ngh·ªÅ ch√≠nh": "N/A",
                }
            
            results.append(data)
            delay = random.randint(3, 5)
            st.write(f"‚è≥ Ch·ªù {delay} gi√¢y tr∆∞·ªõc khi ti·∫øp t·ª•c...\n")
            time.sleep(delay)

        df = pd.DataFrame(results)
        
        # Hi·ªÉn th·ªã DataFrame
        st.dataframe(df)

        # Xu·∫•t file Excel ƒë·ªÉ t·∫£i v·ªÅ
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False, sheet_name="Company Info")
            writer.close()
        
        output.seek(0)
        
        st.download_button(
            label="üì• T·∫£i file Excel",
            data=output,
            file_name="company_info.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

